{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c5fe2c",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47abe06",
   "metadata": {},
   "source": [
    "## 1.1 Load AG News data from FastAi repo in local directory\n",
    "https://registry.opendata.aws/fast-ai-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../news_data/train.csv'\n",
    "test_file = '../news_data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272a7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c031959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    columns = ['topic', 'head', 'body']\n",
    "    df = pd.read_csv(file, header=None)\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a5aca",
   "metadata": {},
   "source": [
    "### Combine datasets to preprocess together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8ddf3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 120000\n",
      "Test set: 7600\n",
      "Test size ratio = 0.06\n"
     ]
    }
   ],
   "source": [
    "train = load_data(train_file)\n",
    "train['set'] = np.array([1,]*len(train))\n",
    "print(f'Train set: {len(train)}')\n",
    "\n",
    "test = load_data(test_file)\n",
    "test['set'] = np.array([0,]*len(test))\n",
    "print(f'Test set: {len(test)}')\n",
    "\n",
    "ratio = round(len(test)/(len(train)+len(test)), 3)\n",
    "print(f'Test size ratio = {ratio}')\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "13f39e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>head</th>\n",
       "      <th>body</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                               head  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                body  set  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...    1  \n",
       "1  Reuters - Private investment firm Carlyle Grou...    1  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...    1  \n",
       "3  Reuters - Authorities have halted oil export\\f...    1  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...    1  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a0ad2",
   "metadata": {},
   "source": [
    "Topic labels:\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0864bc8",
   "metadata": {},
   "source": [
    "## 1.2 Import GloVe word vectors from Stanford NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0d169",
   "metadata": {},
   "source": [
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. <a href=\"https://nlp.stanford.edu/pubs/glove.pdf\">GloVe: Global Vectors for Word Representation. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a6457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = '../../preprocessing/'\n",
    "glove_zip = 'glove.6B.zip'\n",
    "glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "# Choose n-dimensional vector\n",
    "v_size = 100\n",
    "glove_file = f'glove.6B.{v_size}d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2de3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded and extracted at ../../preprocessing/\n"
     ]
    }
   ],
   "source": [
    "def download_and_unzip(url, local_dir, zip_name):\n",
    "    import os\n",
    "    if zip_name[:-4] in os.listdir(local_dir):\n",
    "        print(f'File already downloaded and extracted at {local_dir}')\n",
    "        return None\n",
    "    import requests, zipfile\n",
    "    r = requests.get(url, stream=True)\n",
    "    zip_file = os.path.join(local_dir, zip_name)\n",
    "    if r.status_code == 200:\n",
    "        with open(zip_file, 'wb') as file:\n",
    "            print(f'Saving zip file to {local_dir}..')\n",
    "            file.write(r.content)\n",
    "            z = zipfile.ZipFile(file)\n",
    "            print(f'Unzipping files to {local_dir}..')\n",
    "            z.extractall(zip_file[:-4])\n",
    "\n",
    "download_and_unzip(glove_url, glove_dir, glove_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d05c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors into memory..\n"
     ]
    }
   ],
   "source": [
    "def load_glove(glove_file, local_dir, glove_zip):\n",
    "    file_name = local_dir + '/' + glove_zip[:-4] + '/' + glove_file\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        index = {}\n",
    "        for line in f:\n",
    "            v = line.split()\n",
    "            word = v.pop(0)\n",
    "            coefs = np.array(v, dtype=\"float32\")\n",
    "            index[word] = coefs\n",
    "        print(f'Loaded {len(index)} word vectors into memory..')\n",
    "        return index\n",
    "\n",
    "glove_vector = load_glove(glove_file, glove_dir, glove_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5ffb4",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c89b72",
   "metadata": {},
   "source": [
    "## 2.1 Encode topic label (One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f41ce52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 4 classes of topic labels..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder = pp.LabelEncoder()\n",
    "encoded_topics = encoder.fit_transform(df['topic'].values)\n",
    "print(f'Encoded {len(encoder.classes_)} classes of topic labels..')\n",
    "encoded_topics = to_categorical(encoded_topics)\n",
    "df['y'] = [row for row in encoded_topics]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bdc43",
   "metadata": {},
   "source": [
    "## 2.2 Tokenize header sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e6c4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 37595 words from 127600 sentences..\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sentence_length = 30\n",
    "\n",
    "docs = df['head'].values\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "encoded_docs = pad_sequences(encoded_docs, maxlen=max_sentence_length, padding='post')\n",
    "print(f'Tokenized {len(t.word_index) + 1} words from {len(encoded_docs)} sentences..')\n",
    "encoded_docs\n",
    "df['X'] = [row for row in encoded_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c1ac6",
   "metadata": {},
   "source": [
    "## 2.3 Create embedding matrix using GloVe vector (100 d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d218084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37595, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros([len(t.word_index) + 1, v_size])\n",
    "for word, i in t.word_index.items():\n",
    "    if word in glove_vector:\n",
    "        embedding_matrix[i] = glove_vector[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbadad9",
   "metadata": {},
   "source": [
    "## 2.4 Train-test-split\n",
    "Using the original split provided by the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "46d6a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = df.set.values\n",
    "size = len(df)\n",
    "X_train = np.asarray([encoded_docs[i] for i in range(size) if split[i]==1], dtype='int32')\n",
    "y_train = np.asarray([encoded_topics[i] for i in range(size) if split[i]==1], dtype='int32')\n",
    "\n",
    "X_test = np.asarray([encoded_docs[i] for i in range(size) if split[i]==0], dtype='int32')\n",
    "y_test = np.asarray([encoded_topics[i] for i in range(size) if split[i]==0], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a8b82d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "egx = X_train[:200]\n",
    "egy = y_train[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22350b82",
   "metadata": {},
   "source": [
    "# 3. Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c34e1f",
   "metadata": {},
   "source": [
    "## 3.1 Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801715e",
   "metadata": {},
   "source": [
    "1. Embedding -> 2. Conv1D -> 3. Pooling1D -> 4. Dense -> 5. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "14d1a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "\n",
    "np.random.seed(42)\n",
    "num_classes = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e61eeb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(t.word_index) + 1\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0f938e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "model.add(Embedding(\n",
    "    num_words,\n",
    "    v_size,\n",
    "    embeddings_initializer='uniform',\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_sentence_length,\n",
    "    trainable=False,\n",
    "    name='embedding'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "36b38ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "model.add(Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    name='conv1d'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "40f545a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "model.add(MaxPooling1D(\n",
    "    pool_size=5,\n",
    "    name='maxpool'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a8b24217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 \n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dropout(0.25,name='dropout1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "30446564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "model.add(Dense(\n",
    "    128, \n",
    "    activation='relu',\n",
    "    name='dense1'\n",
    "))\n",
    "model.add(Dropout(0.25,name='dropout2'))\n",
    "model.add(Dense(\n",
    "    64, \n",
    "    activation='relu',\n",
    "    name='dense2'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "24df3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "model.add(Dense(\n",
    "    num_classes,\n",
    "    activation='softmax',\n",
    "    name='output'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9e3bb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "# opt = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[keras.metrics.AUC(), 'acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d62479",
   "metadata": {},
   "source": [
    "## 3.2 Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a031e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7500/7500 - 8s - loss: 0.2559 - auc_7: 0.9875 - acc: 0.9086 - 8s/epoch - 1ms/step\n",
      "Epoch 2/8\n",
      "7500/7500 - 8s - loss: 0.2543 - auc_7: 0.9876 - acc: 0.9102 - 8s/epoch - 1ms/step\n",
      "Epoch 3/8\n",
      "7500/7500 - 8s - loss: 0.2525 - auc_7: 0.9879 - acc: 0.9101 - 8s/epoch - 1ms/step\n",
      "Epoch 4/8\n",
      "7500/7500 - 8s - loss: 0.2508 - auc_7: 0.9880 - acc: 0.9110 - 8s/epoch - 1ms/step\n",
      "Epoch 5/8\n",
      "7500/7500 - 8s - loss: 0.2500 - auc_7: 0.9881 - acc: 0.9109 - 8s/epoch - 1ms/step\n",
      "Epoch 6/8\n",
      "7500/7500 - 8s - loss: 0.2479 - auc_7: 0.9883 - acc: 0.9114 - 8s/epoch - 1ms/step\n",
      "Epoch 7/8\n",
      "7500/7500 - 8s - loss: 0.2475 - auc_7: 0.9882 - acc: 0.9119 - 8s/epoch - 1ms/step\n",
      "Epoch 8/8\n",
      "7500/7500 - 8s - loss: 0.2468 - auc_7: 0.9884 - acc: 0.9120 - 8s/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14dd0aa2f70>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=16, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "20e9f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.38904\n",
      "auc_7: 0.97480\n",
      "acc: 0.87711\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "for i, m in enumerate(model.metrics_names):\n",
    "    print(f'{m}: {scores[i]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff781a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
